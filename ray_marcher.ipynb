{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c049fbe6fd9e585f4bc0fa32eb4f4ecb4c54177d2a14c4a274269393dfb5a8a5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from mymodel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raymarcher(nn.Module):\n",
    "    def __init__(self,\n",
    "             num_feature_channels,\n",
    "             raymarch_steps):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_feature_channels = num_feature_channels\n",
    "        self.steps = raymarch_steps\n",
    "\n",
    "        hidden_size = 16\n",
    "        self.lstm = nn.LSTMCell(input_size=self.n_feature_channels,\n",
    "                                hidden_size=hidden_size)\n",
    "\n",
    "        self.lstm.apply(init_recurrent_weights)\n",
    "        lstm_forget_gate_init(self.lstm)\n",
    "\n",
    "        self.out_layer = nn.Linear(hidden_size, 1)\n",
    "#        self.counter = 0\n",
    "\n",
    "    def forward(self,\n",
    "            cam2world,\n",
    "            phi,\n",
    "            uv,\n",
    "            intrinsics):\n",
    "        batch_size, num_samples, _ = uv.shape\n",
    "        #log = list()\n",
    "\n",
    "        ray_dirs = get_ray_directions(\n",
    "            uv,\n",
    "            cam2world=cam2world,\n",
    "            intrinsics=intrinsics)\n",
    "\n",
    "        initial_depth = torch.zeros((batch_size, num_samples, 1))\\\n",
    "            .normal_(mean=0.05, std=5e-4)\\\n",
    "            .to(device)\n",
    "        init_world_coords = \\\n",
    "            world_from_xy_depth(\n",
    "            uv,\n",
    "            initial_depth,\n",
    "            intrinsics=intrinsics,\n",
    "            cam2world=cam2world)\n",
    "\n",
    "        world_coords = [init_world_coords]\n",
    "        depths = [initial_depth]\n",
    "        states = [None]\n",
    "\n",
    "        for step in range(self.steps):\n",
    "            v = phi(world_coords[-1])\n",
    "\n",
    "            state = self.lstm(v.view(-1, self.n_feature_channels), states[-1])\n",
    "\n",
    "            if state[0].requires_grad:\n",
    "                state[0].register_hook(lambda x: x.clamp(min=-10, max=10))\n",
    "\n",
    "            signed_distance = self.out_layer(state[0]).view(batch_size, num_samples, 1)\n",
    "            new_world_coords = world_coords[-1] + ray_dirs * signed_distance\n",
    "\n",
    "            states.append(state)\n",
    "            world_coords.append(new_world_coords)\n",
    "\n",
    "            depth = depth_from_world(world_coords[-1], cam2world)\n",
    "\n",
    "            if self.training:\n",
    "                print(\"Raymarch step %d: Min depth %0.6f, max depth %0.6f\" %\n",
    "                      (step, depths[-1].min().detach().cpu().numpy(), depths[-1].max().detach().cpu().numpy()))\n",
    "\n",
    "            depths.append(depth)\n",
    "\n",
    "        log = None\n",
    "        return world_coords[-1], depths[-1], log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}